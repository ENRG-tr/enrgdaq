<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>enrgdaq.daq.jobs.store.root API documentation</title>
<meta name="description" content="">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>enrgdaq.daq.jobs.store.root</code></h1>
</header>
<section id="section-intro">
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT"><code class="flex name class">
<span>class <span class="ident">DAQJobStoreROOT</span></span>
<span>(</span><span>config: Any, **kwargs)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DAQJobStoreROOT(DAQJobStore):
    &#34;&#34;&#34;
    ROOT file store with buffered writes for high throughput.

    Buffers incoming data and writes to ROOT in batches to minimize
    TBasket overhead. Per uproot docs, each extend() should be at least
    100 kB per branch for efficient writing.
    &#34;&#34;&#34;

    config_type = DAQJobStoreROOTConfig
    config: DAQJobStoreROOTConfig
    allowed_store_config_types = [DAQJobStoreConfigROOT]
    allowed_message_in_types = [DAQJobMessageStoreTabular, DAQJobMessageStorePyArrow]

    _open_files: dict[str, uproot.WritableDirectory]
    _open_trees: dict[str, dict[str, uproot.WritableTree]]
    _buffers: dict[tuple[str, str], TreeBuffer]  # (file_path, tree_name) -&gt; buffer
    _resolved_paths: dict[str, str]  # original_path -&gt; resolved_path

    def __init__(self, config: Any, **kwargs):
        super().__init__(config, **kwargs)

        self._open_files = {}
        self._open_trees = {}
        self._buffers = {}
        self._resolved_paths = {}

    def start(self):
        while not self._has_been_freed:
            messages_processed = self.consume_all()
            self._flush_ready_buffers()

            if not messages_processed:
                time.sleep(0.001)

    def handle_message(
        self, message: DAQJobMessageStoreTabular | DAQJobMessageStorePyArrow
    ) -&gt; bool:
        super().handle_message(message)

        # Convert message to PyArrow table
        if isinstance(message, DAQJobMessageStorePyArrow):
            table = message.get_table()
            message.release()
            if table.num_rows == 0:
                return True
        else:
            # Convert tabular data to PyArrow table
            data_columns = message.data_columns
            if not data_columns:
                self._logger.warning(
                    &#34;Received tabular message with no data_column, DAQJobStoreROOT does not support that.&#34;
                )
                return True
            table = pa.table(data_columns)

        store_config = cast(DAQJobStoreConfigROOT, message.store_config.root)
        original_path = modify_file_path(
            store_config.file_path, store_config.add_date, message.tag
        )
        original_path = os.path.join(self.config.out_dir, original_path)

        # Resolve file path (use cached resolution or find available path)
        if original_path in self._resolved_paths:
            file_path = self._resolved_paths[original_path]
        else:
            file_path = _get_available_file_path(
                original_path, set(self._resolved_paths.values())
            )
            self._resolved_paths[original_path] = file_path
            if file_path != original_path:
                self._logger.info(
                    f&#34;File &#39;{original_path}&#39; exists, using &#39;{file_path}&#39; instead&#34;
                )

        tree_name = store_config.tree_name

        # Get or create buffer for this file/tree
        buffer_key = (file_path, tree_name)
        if buffer_key not in self._buffers:
            self._buffers[buffer_key] = TreeBuffer(store_config=store_config)

        buffer = self._buffers[buffer_key]

        # Add table to buffer
        buffer.tables.append(table)
        buffer.total_bytes += table.nbytes

        # Check if we should flush immediately (buffer too large)
        if buffer.total_bytes &gt;= self.config.buffer_size_bytes:
            self._flush_buffer(file_path, tree_name)

        return True

    def _flush_ready_buffers(self):
        &#34;&#34;&#34;Flush buffers that have exceeded the time threshold.&#34;&#34;&#34;
        now = datetime.now()
        buffers_to_flush = []

        for (file_path, tree_name), buffer in self._buffers.items():
            if not buffer.tables:
                continue

            time_since_flush = (now - buffer.last_flush_time).total_seconds()
            if time_since_flush &gt;= self.config.flush_interval_seconds:
                buffers_to_flush.append((file_path, tree_name))

        for file_path, tree_name in buffers_to_flush:
            self._flush_buffer(file_path, tree_name)

    def _flush_buffer(self, file_path: str, tree_name: str):
        &#34;&#34;&#34;Flush buffered data to ROOT file.&#34;&#34;&#34;
        buffer_key = (file_path, tree_name)
        if buffer_key not in self._buffers:
            return

        buffer = self._buffers[buffer_key]
        if not buffer.tables:
            return

        store_config = buffer.store_config
        assert store_config is not None, &#34;Buffer has no store_config&#34;

        # Combine all buffered tables into one
        combined_table = pa.concat_tables(buffer.tables)

        # Convert to dict of numpy arrays for uproot
        data_to_write = {
            col_name: combined_table.column(col_name).to_numpy()
            for col_name in combined_table.column_names
        }

        # Open or get ROOT file
        if file_path not in self._open_files or self._open_files[file_path].closed:
            dir_name = os.path.dirname(file_path)
            if dir_name:
                os.makedirs(dir_name, exist_ok=True)

            if os.path.exists(file_path):
                root_file = uproot.update(file_path)
            else:
                root_file = uproot.recreate(
                    file_path,
                    compression=ROOT_COMPRESSION_TYPES[store_config.compression_type](
                        level=store_config.compression_level
                    ),
                )

            self._open_files[file_path] = root_file
            self._logger.debug(f&#34;Opened file {file_path}&#34;)
        else:
            root_file = self._open_files[file_path]

        # Create or get tree
        # Check our cache first - if we have the tree cached, use it
        if file_path in self._open_trees and tree_name in self._open_trees[file_path]:
            tree = self._open_trees[file_path][tree_name]
        elif tree_name not in root_file:
            # Tree doesn&#39;t exist, create it
            tree = root_file.mktree(
                tree_name,
                {
                    k: v.dtype
                    for k, v in data_to_write.items()
                    if isinstance(v, ndarray)
                },
            )
            self._logger.debug(f&#34;Created tree {tree_name}&#34;)
            if file_path not in self._open_trees:
                self._open_trees[file_path] = {}
            self._open_trees[file_path][tree_name] = tree
        else:
            # Tree exists in file but not in our cache - this shouldn&#39;t happen
            # in normal operation. It can occur if a file was left from a previous
            # run. The caller should clean up output files before starting.
            raise RuntimeError(
                f&#34;Tree &#39;{tree_name}&#39; already exists in file &#39;{file_path}&#39; but was not created &#34;
                f&#34;by this process. Please remove the file and try again.&#34;
            )

        # Write the combined data in one extend() call
        start_time = time.time()
        tree.extend(data_to_write)
        write_time = time.time() - start_time

        self._logger.debug(
            f&#34;Flushed {len(buffer.tables)} messages ({buffer.total_bytes / 1_000_000:.2f} MB, &#34;
            f&#34;{combined_table.num_rows} rows) to {tree_name} in {write_time:.3f}s&#34;
        )

        # Clear buffer
        buffer.tables.clear()
        buffer.total_bytes = 0
        buffer.last_flush_time = datetime.now()

    def _flush_all_buffers(self):
        &#34;&#34;&#34;Flush all buffers regardless of thresholds.&#34;&#34;&#34;
        for file_path, tree_name in list(self._buffers.keys()):
            self._flush_buffer(file_path, tree_name)

    def __del__(self):
        # Flush any remaining buffered data
        try:
            self._flush_all_buffers()
        except Exception:
            pass

        # Ensure all files are properly closed on exit
        for root_file in self._open_files.values():
            if not root_file.closed:
                root_file.close()
        self._open_files.clear()

        super().__del__()</code></pre>
</details>
<div class="desc"><p>ROOT file store with buffered writes for high throughput.</p>
<p>Buffers incoming data and writes to ROOT in batches to minimize
TBasket overhead. Per uproot docs, each extend() should be at least
100 kB per branch for efficient writing.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="enrgdaq.daq.store.base.DAQJobStore" href="../../store/base.html#enrgdaq.daq.store.base.DAQJobStore">DAQJobStore</a></li>
<li><a title="enrgdaq.daq.base.DAQJob" href="../../base.html#enrgdaq.daq.base.DAQJob">DAQJob</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.allowed_message_in_types"><code class="name">var <span class="ident">allowed_message_in_types</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.allowed_store_config_types"><code class="name">var <span class="ident">allowed_store_config_types</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.config"><code class="name">var <span class="ident">config</span> : <a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig">DAQJobStoreROOTConfig</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.config_type"><code class="name">var <span class="ident">config_type</span> : type[<a title="enrgdaq.daq.models.DAQJobConfig" href="../../models.html#enrgdaq.daq.models.DAQJobConfig">DAQJobConfig</a>]</code></dt>
<dd>
<div class="desc"><p>DAQJobConfig is the base configuration class for DAQJobs.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>verbosity</code></strong> :&ensp;<code>LogVerbosity</code></dt>
<dd>The verbosity level for logging. Defaults to LogVerbosity.INFO.</dd>
<dt><strong><code>remote_config</code></strong> :&ensp;<code>Optional[DAQRemoteConfig]</code></dt>
<dd>The remote configuration for the DAQ job. Defaults to an instance of DAQRemoteConfig.</dd>
<dt><strong><code>daq_job_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the DAQ job.</dd>
<dt><strong><code>daq_job_unique_id</code></strong> :&ensp;<code>str</code></dt>
<dd>The unique identifier for the DAQ job.</dd>
<dt><strong><code>use_shm_when_possible</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use shared memory when possible. It is guaranteed to never be used when set to False, although not guaranteed when set to True.</dd>
</dl></div>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="enrgdaq.daq.store.base.DAQJobStore" href="../../store/base.html#enrgdaq.daq.store.base.DAQJobStore">DAQJobStore</a></b></code>:
<ul class="hlist">
<li><code><a title="enrgdaq.daq.store.base.DAQJobStore.handle_message" href="../../base.html#enrgdaq.daq.base.DAQJob.handle_message">handle_message</a></code></li>
<li><code><a title="enrgdaq.daq.store.base.DAQJobStore.start" href="../../store/base.html#enrgdaq.daq.store.base.DAQJobStore.start">start</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig"><code class="flex name class">
<span>class <span class="ident">DAQJobStoreROOTConfig</span></span>
<span>(</span><span>out_dir: str = 'out/',<br>buffer_size_bytes: int = 5000000,<br>flush_interval_seconds: float = 1.0,<br>*,<br>daq_job_type: str,<br>verbosity: <a title="enrgdaq.models.LogVerbosity" href="../../../models.html#enrgdaq.models.LogVerbosity">LogVerbosity</a> = LogVerbosity.INFO,<br>remote_config: <a title="enrgdaq.daq.models.DAQRemoteConfig" href="../../models.html#enrgdaq.daq.models.DAQRemoteConfig">DAQRemoteConfig</a> = &lt;factory&gt;,<br>daq_job_unique_id: str | None = None,<br>use_shm_when_possible: bool = True)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DAQJobStoreROOTConfig(DAQJobConfig):
    out_dir: str = &#34;out/&#34;
    buffer_size_bytes: int = BUFFER_SIZE_THRESHOLD_BYTES
    flush_interval_seconds: float = BUFFER_FLUSH_INTERVAL_SECONDS</code></pre>
</details>
<div class="desc"><p>DAQJobConfig is the base configuration class for DAQJobs.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>verbosity</code></strong> :&ensp;<code>LogVerbosity</code></dt>
<dd>The verbosity level for logging. Defaults to LogVerbosity.INFO.</dd>
<dt><strong><code>remote_config</code></strong> :&ensp;<code>Optional[DAQRemoteConfig]</code></dt>
<dd>The remote configuration for the DAQ job. Defaults to an instance of DAQRemoteConfig.</dd>
<dt><strong><code>daq_job_type</code></strong> :&ensp;<code>str</code></dt>
<dd>The type of the DAQ job.</dd>
<dt><strong><code>daq_job_unique_id</code></strong> :&ensp;<code>str</code></dt>
<dd>The unique identifier for the DAQ job.</dd>
<dt><strong><code>use_shm_when_possible</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether to use shared memory when possible. It is guaranteed to never be used when set to False, although not guaranteed when set to True.</dd>
</dl></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="enrgdaq.daq.models.DAQJobConfig" href="../../models.html#enrgdaq.daq.models.DAQJobConfig">DAQJobConfig</a></li>
<li>msgspec.Struct</li>
<li>msgspec._core._StructMixin</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.buffer_size_bytes"><code class="name">var <span class="ident">buffer_size_bytes</span> : int</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DAQJobStoreROOTConfig(DAQJobConfig):
    out_dir: str = &#34;out/&#34;
    buffer_size_bytes: int = BUFFER_SIZE_THRESHOLD_BYTES
    flush_interval_seconds: float = BUFFER_FLUSH_INTERVAL_SECONDS</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.flush_interval_seconds"><code class="name">var <span class="ident">flush_interval_seconds</span> : float</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DAQJobStoreROOTConfig(DAQJobConfig):
    out_dir: str = &#34;out/&#34;
    buffer_size_bytes: int = BUFFER_SIZE_THRESHOLD_BYTES
    flush_interval_seconds: float = BUFFER_FLUSH_INTERVAL_SECONDS</code></pre>
</details>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.out_dir"><code class="name">var <span class="ident">out_dir</span> : str</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class DAQJobStoreROOTConfig(DAQJobConfig):
    out_dir: str = &#34;out/&#34;
    buffer_size_bytes: int = BUFFER_SIZE_THRESHOLD_BYTES
    flush_interval_seconds: float = BUFFER_FLUSH_INTERVAL_SECONDS</code></pre>
</details>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.TreeBuffer"><code class="flex name class">
<span>class <span class="ident">TreeBuffer</span></span>
<span>(</span><span>tables: list[pyarrow.lib.Table] = &lt;factory&gt;,<br>total_bytes: int = 0,<br>last_flush_time: datetime.datetime = &lt;factory&gt;,<br>store_config: <a title="enrgdaq.daq.store.models.DAQJobStoreConfigROOT" href="../../store/models.html#enrgdaq.daq.store.models.DAQJobStoreConfigROOT">DAQJobStoreConfigROOT</a> | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class TreeBuffer:
    &#34;&#34;&#34;Buffer for accumulating data before writing to ROOT tree.&#34;&#34;&#34;

    tables: list[pa.Table] = field(default_factory=list)
    total_bytes: int = 0
    last_flush_time: datetime = field(default_factory=datetime.now)
    store_config: DAQJobStoreConfigROOT | None = None</code></pre>
</details>
<div class="desc"><p>Buffer for accumulating data before writing to ROOT tree.</p></div>
<h3>Instance variables</h3>
<dl>
<dt id="enrgdaq.daq.jobs.store.root.TreeBuffer.last_flush_time"><code class="name">var <span class="ident">last_flush_time</span> : datetime.datetime</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.TreeBuffer.store_config"><code class="name">var <span class="ident">store_config</span> : <a title="enrgdaq.daq.store.models.DAQJobStoreConfigROOT" href="../../store/models.html#enrgdaq.daq.store.models.DAQJobStoreConfigROOT">DAQJobStoreConfigROOT</a> | None</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.TreeBuffer.tables"><code class="name">var <span class="ident">tables</span> : list[pyarrow.lib.Table]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="enrgdaq.daq.jobs.store.root.TreeBuffer.total_bytes"><code class="name">var <span class="ident">total_bytes</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="enrgdaq.daq.jobs.store" href="index.html">enrgdaq.daq.jobs.store</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOT">DAQJobStoreROOT</a></code></h4>
<ul class="">
<li><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.allowed_message_in_types" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.allowed_message_in_types">allowed_message_in_types</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.allowed_store_config_types" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.allowed_store_config_types">allowed_store_config_types</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.config" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.config">config</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.config_type" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOT.config_type">config_type</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig">DAQJobStoreROOTConfig</a></code></h4>
<ul class="">
<li><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.buffer_size_bytes" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.buffer_size_bytes">buffer_size_bytes</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.flush_interval_seconds" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.flush_interval_seconds">flush_interval_seconds</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.out_dir" href="#enrgdaq.daq.jobs.store.root.DAQJobStoreROOTConfig.out_dir">out_dir</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="enrgdaq.daq.jobs.store.root.TreeBuffer" href="#enrgdaq.daq.jobs.store.root.TreeBuffer">TreeBuffer</a></code></h4>
<ul class="">
<li><code><a title="enrgdaq.daq.jobs.store.root.TreeBuffer.last_flush_time" href="#enrgdaq.daq.jobs.store.root.TreeBuffer.last_flush_time">last_flush_time</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.TreeBuffer.store_config" href="#enrgdaq.daq.jobs.store.root.TreeBuffer.store_config">store_config</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.TreeBuffer.tables" href="#enrgdaq.daq.jobs.store.root.TreeBuffer.tables">tables</a></code></li>
<li><code><a title="enrgdaq.daq.jobs.store.root.TreeBuffer.total_bytes" href="#enrgdaq.daq.jobs.store.root.TreeBuffer.total_bytes">total_bytes</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
